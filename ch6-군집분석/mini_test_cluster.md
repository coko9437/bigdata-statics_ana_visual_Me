
---

### **Ch6-2 코드 종합 설명**

이 코드는 데이터 과학에서 "Hello, World!"와 같은 위상을 가진 **붓꽃(Iris) 데이터셋**을 사용하여 **K-평균 군집 분석**을 수행하는 전 과정을 보여줍니다. 4개의 꽃잎/꽃받침 측정치(특징)만을 이용해 데이터 내에 자연적으로 존재하는 그룹(군집)을 찾아내는 것이 목표입니다. 이 예제의 핵심은 **정답(실제 품종)을 사용하지 않고** 군집화를 수행한 뒤, 그 결과가 **얼마나 실제 품종과 일치하는지 비교**하여 K-평균 알고리즘의 성능을 객관적으로 평가하는 데 있습니다.

1.  **데이터 준비 및 전처리:** `scikit-learn`에 내장된 Iris 데이터셋을 불러오고, 거리 기반 알고리즘인 K-평균의 성능을 위해 모든 특징을 동일한 스케일로 맞추는 **표준화(Standardization)**를 수행합니다.
2.  **최적 군집 수(K) 탐색:** 군집 분석의 핵심 과제인 '몇 개의 그룹으로 나눌 것인가?'에 답하기 위해 **엘보우 방법(Elbow Method)**과 **실루엣 분석(Silhouette Analysis)**을 사용하여 최적의 K값을 탐색합니다. Iris 데이터는 3개의 품종이 있으므로, K=3이 최적일 것으로 예상하고 이를 확인하는 과정입니다.
3.  **군집화 및 시각화:** 결정된 최적 K값(K=3)으로 K-평균 모델을 학습시켜 각 데이터 포인트를 3개의 군집 중 하나에 할당합니다. 4차원의 원본 데이터를 사람이 인지할 수 있도록 **주성분 분석(PCA)**을 이용해 2차원으로 차원을 축소한 후, 결과를 **산점도(Scatter Plot)**로 시각화하여 군집이 얼마나 잘 분리되었는지 확인합니다.
4.  **결과 분석 및 평가:** 각 군집의 특징(평균 측정치)을 분석하여 어떤 그룹인지 해석하고, 새로운 샘플 데이터가 어떤 군집에 속할지 예측합니다. 마지막으로, 처음부터 알고 있었지만 사용하지 않았던 **실제 품종 라벨(`species`)**과 K-평균이 예측한 **군집 라벨(`cluster`)**을 비교하여 **ARI(Adjusted Rand Index)**와 **NMI(Normalized Mutual Information)**라는 외부 평가 지표로 모델의 정확도를 정량적으로 측정합니다.

---

### **Part 1: 데이터 준비 및 전처리**

정확한 군집 분석을 위해 데이터를 불러오고, 알고리즘에 적합한 형태로 가공하는 첫 단계입니다.

#### **1. 코드, 문법 및 개별 설명**

```python
# %%
# 필요한 라이브러리 임포트
import pandas as pd
from sklearn.datasets import load_iris
from sklearn.preprocessing import StandardScaler

# scikit-learn에 내장된 Iris 데이터셋을 로드
iris = load_iris()
# iris.data (특징 데이터)와 iris.feature_names (컬럼 이름)를 이용해 DataFrame 생성
df = pd.DataFrame(iris.data, columns=iris.feature_names)

# 평가를 위해 실제 품종 정보('target')를 'species' 컬럼으로 추가
# 0: Setosa, 1: Versicolor, 2: Virginica
df["species"] = iris.target

# --- 데이터 전처리 ---
# 군집 분석에 사용할 독립 변수(X) 선택
# 'species'는 정답이므로 분석(학습) 과정에서는 제외
X = df.drop(columns=["species"])

# 데이터 표준화(Standardization)
# StandardScaler 객체 생성
scaler = StandardScaler()
# scaler가 X 데이터의 평균과 표준편차를 학습(fit)하고,
# 그 기준으로 데이터를 변환(transform)하여 X_scaled에 저장
X_scaled = scaler.fit_transform(X)
```

#### **2. 해당 설명**

이 파트의 핵심은 **비지도 학습의 개념**과 **데이터 스케일링**의 중요성을 이해하는 것입니다.

*   **비지도 학습:** `df.drop(columns=["species"])` 코드는 "정답을 보지 않고 데이터의 순수한 특징만으로 그룹을 나눠보겠다"는 비지도 학습의 철학을 보여줍니다. 우리는 이미 3개의 품종이 있다는 것을 알지만, K-평균 알고리즘에게는 이 정보를 숨기는 것입니다.
*   **데이터 표준화:** K-평균은 각 데이터 포인트 간의 유클리드 거리를 기반으로 동작합니다. 만약 어떤 특징(예: 꽃잎 길이)이 다른 특징(예: 꽃받침 너비)보다 값의 범위(스케일)가 훨씬 크다면, 거리 계산 시 그 특징이 과도하게 큰 영향을 미치게 됩니다. `StandardScaler`는 모든 특징을 **평균 0, 표준편차 1**로 변환하여, 모든 특징이 동등한 중요도로 거리에 반영되도록 만들어줍니다. 이는 거리 기반 알고리즘의 성능에 필수적인 전처리 과정입니다.

#### **3. 응용 가능한 예제**

**"고객 신용도 평가 모델링"**

고객의 `연봉`, `나이`, `부채 비율` 데이터를 이용해 신용 등급을 예측할 때, '연봉'의 단위(수천만 원)는 '나이'(수십)나 '부채 비율'(0~1)보다 훨씬 큽니다. 표준화를 적용하지 않으면 모델이 '연봉'이라는 특징에만 거의 전적으로 의존하게 될 수 있으므로, 반드시 스케일링을 통해 각 변수의 영향력을 동등하게 맞춰주어야 합니다.

#### **4. 추가하고 싶은 내용 (fit vs transform vs fit_transform)**

*   `fit()`: 스케일링에 필요한 정보(평균, 표준편차 등)를 데이터로부터 학습합니다.
*   `transform()`: 학습된 정보를 바탕으로 데이터를 실제로 변환합니다.
*   `fit_transform()`: `fit()`과 `transform()`을 한 번에 수행합니다. (훈련 데이터에 사용)

나중에 새로운 데이터(예: 샘플 데이터)를 예측할 때는, 반드시 **훈련 데이터로 `fit`했던 `scaler` 객체를 그대로 사용**하여 `transform()`만 수행해야 합니다. 이는 일관된 기준으로 데이터를 변환하기 위함입니다.

#### **5. 심화 내용 (데이터의 가정)**

K-평균 알고리즘은 각 군집이 **원형(spherical)이며 비슷한 분산**을 가질 때 가장 잘 작동합니다. 데이터 표준화는 각 특징의 분산을 1로 만들어주므로, 이러한 가정을 어느 정도 만족시키는 데 도움을 줍니다.

---

### **Part 2: 최적 군집 수(K) 탐색**

데이터를 가장 의미 있는 개수의 그룹으로 나누기 위한 최적의 K를 찾는 과정입니다.

#### **1. 코드, 문법 및 개별 설명**

```python
# %%
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score, silhouette_samples
import matplotlib.cm as cm

# --- 1. 엘보우 방법 ---
# WCSS(Within-Cluster Sum of Squares) 값을 저장할 리스트
wcss = []
# K를 1부터 10까지 테스트
for k in range(1, 11):
    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)
    kmeans.fit(X_scaled)
    # inertia_ 속성: K-Means 모델의 WCSS(군집 내 오차 제곱합) 값
    wcss.append(kmeans.inertia_)

# 엘보우 그래프 시각화
plt.figure(figsize=(8, 5))
plt.plot(range(1, 11), wcss, marker="o", linestyle="--")
plt.title("엘보우 방법을 통한 최적 K 찾기")
# ... (그래프 설정) ...
plt.show() # 결과: K=3에서 팔꿈치 모양이 뚜렷하게 나타남

# --- 2. 실루엣 분석 ---
# K=3일 때의 실루엣 점수와 그래프를 시각화하는 함수 호출
silhouetteViz(3, X_scaled)
```

#### **2. 해당 설명**

이 파트에서는 최적의 K를 찾기 위한 두 가지 대표적인 방법, **엘보우 방법**과 **실루엣 분석**을 사용합니다.

*   **엘보우 방법:** K가 증가함에 따라 각 군집 내 데이터들의 응집도(얼마나 뭉쳐있는지)를 나타내는 `WCSS`는 계속 감소합니다. 하지만 **그래프가 팔꿈치처럼 급격히 꺾이는 지점**은, K를 그 이상으로 늘려도 얻는 이득(응집도 향상)이 크지 않다는 것을 의미합니다. Iris 데이터에서는 K=3에서 뚜렷한 팔꿈치가 관찰되므로, K=3이 최적의 군집 수일 가능성이 매우 높습니다.
*   **실루엣 분석:** 엘보우 방법이 때로는 모호할 수 있기 때문에, 실루엣 분석을 통해 결과를 보강합니다. 실루엣 그래프는 각 군집이 얼마나 잘 분리되고 응집되었는지를 시각적으로 보여줍니다.
    *   **해석:** K=3일 때, (1) 전체 평균 실루엣 점수가 0.46으로 준수하며, (2) 3개의 군집 두께가 비교적 균일하고, (3) 음수 값을 갖는 데이터가 거의 없습니다. 이는 K=3으로 군집화했을 때의 결과가 매우 안정적이고 신뢰할 수 있음을 의미합니다.

이 두 가지 방법을 통해, 우리는 "데이터를 3개의 그룹으로 나누는 것이 가장 합리적이다"라는 결론에 도달할 수 있습니다.

#### **3. 응용 가능한 예제**

**"온라인 뉴스 기사 그룹핑"**

수만 건의 뉴스 기사를 내용에 따라 몇 개의 주제로 묶을지 결정해야 할 때, 엘보우 방법과 실루엣 분석을 통해 '정치', '경제', '사회', 'IT/과학', '스포츠' 등과 같이 가장 의미 있는 주제의 개수(K)를 찾아낼 수 있습니다.

#### **4. 추가하고 싶은 내용 (`n_init` 파라미터)**

`KMeans`의 `n_init` 파라미터는 초기 중심점 위치를 몇 번이나 다르게 설정하여 알고리즘을 실행할지 결정합니다. K-평균은 초기 중심점 위치에 따라 결과가 달라질 수 있기 때문에, 여러 번 실행하여 그중 가장 좋은(WCSS가 가장 낮은) 결과를 최종 모델로 선택합니다. `n_init=10`(기본값)은 10번 시도하겠다는 의미로, 모델의 안정성을 높여줍니다.

#### **5. 심화 내용 (군집 수의 주관성)**

최적의 K를 찾는 방법들은 수학적 지표를 제공하지만, 최종 결정은 분석가의 **도메인 지식(Domain Knowledge)**과 **분석의 목적**에 따라 달라질 수 있습니다. 예를 들어, 고객 세분화에서 실루엣 점수는 K=4가 가장 높더라도, 마케팅 전략상 6개의 그룹으로 나누는 것이 더 유용하다면 K=6을 선택할 수도 있습니다.

---

### **Part 3: K-평균 군집화 및 시각화**

결정된 K값으로 실제 군집화를 수행하고, 사람이 이해할 수 있도록 그 결과를 시각화하는 단계입니다.

#### **1. 코드, 문법 및 개별 설명**

```python
# %%
from sklearn.decomposition import PCA
import seaborn as sns

# 최적의 K=3으로 설정하여 K-Means 모델을 생성하고 학습 및 예측을 동시에 수행
kmeans = KMeans(n_clusters=3, random_state=42, n_init=10)
# fit_predict(): X_scaled 데이터로 모델을 학습(fit)하고, 각 데이터가 어떤 군집에 속하는지 예측(predict)
df["cluster"] = kmeans.fit_predict(X_scaled)

# --- PCA를 이용한 차원 축소 및 시각화 ---
# 4차원 데이터를 2차원으로 축소하기 위해 PCA 객체 생성
# n_components=2: 주성분을 2개만 추출하겠다는 의미
pca = PCA(n_components=2)
# X_scaled 데이터를 2차원으로 차원 축소
X_pca = pca.fit_transform(X_scaled)

# PCA 결과를 시각화를 위해 DataFrame으로 변환
df_pca = pd.DataFrame(X_pca, columns=["PC1", "PC2"])
# 군집 라벨 정보를 PCA 데이터프레임에 추가
df_pca["cluster"] = df["cluster"]

# 산점도를 이용한 군집 시각화
plt.figure(figsize=(8, 6))
# x축은 첫 번째 주성분(PC1), y축은 두 번째 주성분(PC2)
# hue='cluster': 군집 라벨에 따라 점의 색상을 다르게 표시
sns.scatterplot(data=df_pca, x="PC1", y="PC2", hue="cluster", palette="viridis", s=100)
plt.title("K-Means 군집화 결과 (PCA 차원 축소 후)")
plt.show()
```

#### **2. 해당 설명**

이 파트의 핵심은 **PCA(주성분 분석, Principal Component Analysis)**의 역할입니다.

*   **왜 PCA를 사용하는가?** Iris 데이터는 4개의 특징(4차원)을 가지고 있습니다. 우리는 4차원 공간을 직접 볼 수 없기 때문에, 데이터의 구조를 시각적으로 확인하기가 불가능합니다. PCA는 이러한 **고차원 데이터의 분산(정보)을 최대한 보존하면서 저차원(여기서는 2차원)으로 축소**하는 기법입니다.
*   **PC1과 PC2의 의미:** `PC1`(첫 번째 주성분)은 원본 4개 특징의 정보를 가장 많이 함축하고 있는 새로운 축입니다. `PC2`(두 번째 주성분)는 PC1과 직교하면서(관련이 없으면서) 나머지 정보 중 가장 많은 정보를 담고 있는 축입니다.
*   **시각화 해석:** PCA로 변환된 2차원 평면에 군집 결과를 그려보면, 3개의 군집이 뚜렷하게 분리되어 있는 것을 확인할 수 있습니다. 이는 K-평균 알고리즘이 4차원 공간에서 데이터의 구조를 성공적으로 찾아냈음을 시각적으로 보여주는 증거입니다.

#### **3. 응용 가능한 예제**

**"고객 설문조사 데이터 시각화"**

수십 개의 문항으로 이루어진 고객 만족도 설문조사 결과를 분석할 때, PCA를 사용하면 전체 응답 패턴을 2차원 맵에 시각화할 수 있습니다. 이를 통해 '가격에 민감한 그룹', '서비스 품질을 중시하는 그룹' 등 자연스럽게 형성된 고객 군집을 한눈에 파악할 수 있습니다.

#### **4. 추가하고 싶은 내용 (군집 중심점 시각화)**

시각화할 때 각 군집의 중심점(centroid)을 함께 표시하면 군집의 특징을 더 명확하게 이해할 수 있습니다. PCA로 변환된 군집 중심점을 산점도에 다른 모양(예: 별표)으로 추가하여 그릴 수 있습니다.

#### **5. 심화 내용 (PCA의 설명력)**

PCA 객체의 `explained_variance_ratio_` 속성을 확인하면, 각 주성분이 원본 데이터의 분산을 얼마나 설명하는지 알 수 있습니다. 예를 들어 `pca.explained_variance_ratio_`의 합이 0.95라면, 2개의 주성분만으로 원본 4차원 데이터의 정보를 95%나 설명할 수 있다는 의미입니다. 이를 통해 차원 축소로 인한 정보 손실이 어느 정도인지 파악할 수 있습니다.

---

### **Part 4: 결과 분석 및 외부 평가**

군집화 결과를 해석하고, 정답 라벨과의 비교를 통해 모델의 성능을 객관적으로 평가하는 최종 단계입니다.

#### **1. 코드, 문법 및 개별 설명**

```python
# %%
from sklearn.metrics import adjusted_rand_score, normalized_mutual_info_score

# --- 결과 분석 ---
# 1. 각 군집의 특징(평균값) 확인
# 'cluster' 라벨로 그룹화한 뒤, 각 특징의 평균을 계산
cluster_means = df.groupby("cluster").mean()
print(cluster_means)
# 결과 해석: 각 군집이 어떤 붓꽃 품종의 특성과 유사한지 파악 가능
# 예: 0번 군집은 꽃잎/꽃받침 크기가 모두 작음 -> Setosa 품종으로 추정

# --- 새로운 데이터 예측 ---
# ... (샘플 데이터 생성 및 예측 코드 생략) ...
# 핵심 로직:
# 1. scaler.transform(sample_data): 훈련 데이터와 동일한 기준으로 스케일링
# 2. kmeans.predict(sample_data_scaled): 학습된 모델로 어느 군집에 속할지 예측

# --- 외부 평가 (정답과 비교) ---
# ARI (Adjusted Rand Index) 계산
# df['species'] (정답)와 df['cluster'] (예측)가 얼마나 유사한지 측정
ari_score = adjusted_rand_score(df['species'], df['cluster'])

# NMI (Normalized Mutual Information) 계산
nmi_score = normalized_mutual_info_score(df['species'], df['cluster'])

print(f"Adjusted Rand Index (ARI): {ari_score:.4f}")   # 결과: 약 0.88
print(f"Normalized Mutual Information (NMI): {nmi_score:.4f}") # 결과: 약 0.88
```

#### **2. 해당 설명**

이 파트에서는 **내부 평가(Internal Evaluation)**와 **외부 평가(External Evaluation)**의 차이를 이해하는 것이 중요합니다.

*   **내부 평가:** 실루엣 점수처럼 정답 라벨 없이 데이터 자체의 구조만으로 군집의 품질을 평가하는 방법입니다. 이전 `ch6-1` 예제에서는 정답이 없었기 때문에 내부 평가만 가능했습니다.
*   **외부 평가:** **ARI**와 **NMI**처럼 정답 라벨과 군집 결과를 비교하여 얼마나 일치하는지 평가하는 방법입니다. Iris 데이터셋에는 `species`라는 정답이 있기 때문에 외부 평가가 가능합니다.
    *   **ARI / NMI 해석:** 두 지표 모두 1에 가까울수록 완벽하게 일치함을 의미하고, 0에 가까우면 무작위로 할당한 것과 비슷하다는 의미입니다. ARI와 NMI 점수가 모두 약 0.88로 매우 높게 나왔다는 것은, **K-평균 알고리즘이 정답을 전혀 보지 않았음에도 불구하고, 데이터의 특징만으로 실제 붓꽃 품종을 거의 정확하게 구분해냈다**는 놀라운 사실을 의미합니다.

`groupby().mean()` 결과는 각 군집의 특성을 정의하는 데 사용됩니다. 예를 들어, 0번 군집은 다른 군집에 비해 모든 측정치가 작으므로 'Setosa' 품종에 해당한다고 해석할 수 있습니다.

#### **3. 응용 가능한 예제**

**"의료 영상 이미지를 이용한 종양 분류"**

의료 영상 이미지에서 추출한 특징들을 K-평균으로 군집화하여 '양성 종양 의심', '악성 종양 의심' 그룹으로 나눌 수 있습니다. 이후, 조직 검사를 통해 확진된 실제 종양 종류(정답 라벨)와 군집 결과를 비교하여 ARI, NMI 점수를 계산하면, 해당 군집화 방법이 진단에 얼마나 도움이 될 수 있는지 객관적으로 평가할 수 있습니다.

#### **4. 추가하고 싶은 내용 (혼동 행렬, Confusion Matrix)**

군집 결과와 실제 라벨의 관계를 더 직관적으로 보려면 **혼동 행렬(Confusion Matrix)**을 사용하는 것이 매우 효과적입니다. `pd.crosstab(df['species'], df['cluster'])` 코드를 실행하면 어떤 품종이 어떤 군집으로 얼마나 많이 분류되었는지 한눈에 파악할 수 있습니다.

#### **5. 심화 내용 (군집 라벨의 불일치)**

K-평균이 할당하는 군집 라벨(0, 1, 2)과 실제 품종 라벨(0: Setosa, 1: Versicolor, 2: Virginica)의 숫자가 직접적으로 일치하지는 않습니다. 예를 들어, K-평균이 Versicolor를 '군집 0'으로, Setosa를 '군집 1'로 할당할 수도 있습니다. ARI와 NMI는 이러한 라벨 값 자체에는 영향을 받지 않고, '어떤 데이터들이 같은 그룹으로 묶였는가'라는 본질적인 구조의 일치성만을 측정하기 때문에 신뢰할 수 있는 평가 지표입니다.