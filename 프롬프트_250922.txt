코드를 보여주면 너가 해당 코드에 대해서 자세한 설명을 해줄수 있어?
코드가 있다면 거기에 해당하는 코드에 대한 설명과 임폴트, 함수의 의미 등에 대해서 주석을 이용해서 설명을 붙여주고, 해당 예제를 이용한 응용가능한 예제가 있다면 만들어주고, 너가 생각했을때 추가했으면 하는 내용, 좀더 깊이있는 심화문제, 예제, 심화 설명을 붙여주면 좋겠어.
예시코드와 해당하는 설명 예시를 보여줄게.

예시코드 1:
#%% md
# 타이타닉 생존율 분석 해보기
## 상관분석, 상관 계수, 피어슨 상관 계수, 히트맵
### 타이타닉 데이터 seaborn 내장 데이터 셋
### 전처리 비슷하게, null  값을 , 최빈값(가장 빈도가 높은 데이터), 중앙값 대체
### 파이 차트, 갯수 차트, 히트맵, 시각화
### 1) 모든 변수 간 상관계수 구하고, 2) 지정한 두 변수 간 상관계수도 구하기.

## 용어정리
### 상관 분석 : 두 변수가 어떤 선형적 관계에 있는지 분석 하는 방법.
### 두 변수의 관계의 강도를 상관관계라고 함.
### 상관관계의 정도를 나타내는 단위를 모상관 계수 p(파이)를 사용함. by 위키피디아

### 단순 상관 분석: 두 변수 사이의 어느 정도 강한 관계 인지.
### 다중 상관 분석 : 세 개이상의 변수 간 관계의 강도를 측정.

### 상관계수 : + , - , 독립변수가, 종속 변수에 어떠한 영향을 미치는지,
### x 증가시, y도 증가하는지, 아니면 감소하는지?
### 이번에는 수치를 구간을 나눠서, 어느정도 연관성이 있는지 수치화해서, 예측하기.

### 피어슨 상관계수 : 많이 사용하는 예제다.
### ex) 0.0 ~ 0.2 : 상관관계가 거의 없다.
### ex) 0.2 ~ 0.4 : 약한 상관관계가 있다.
### ex) 0.4 ~ 0.6 : 상관관계가 있다.
### ex) 0.6 ~ 0.8 : 강한 상관관계가 있다.
### ex) 0.8 ~ 1.0 : 매우 강한 상관관계가 있다.

#%%
# pclass
# # 탑승권 등급을 나타냅니다.
# 1: 1등실, 2: 2등실, 3: 3등실

# sibsp
# 함께 탑승한 형제자매(siblings)와 배우자(spouse)의 수
# 예: 1이면 형제나 배우자 중 1명이 함께 탑승했다는 뜻

# parch
# 함께 탑승한 부모(parents)와 자녀(children)의 수
# 예: 2이면 부모나 자녀 중 2명이 함께 탑승했다는 뜻

# embarked
# 승선(탑승)한 항구를 의미합니다.
# 일반적으로 C(Cherbourg), Q(Queenstown), S(Southampton) 세 가지 값이 사용됩니다.

# class
# pclass를 문자 형태로 표현한 것(‘First’, ‘Second’, ‘Third’ 등).

# who
# 탑승객을 ‘man’, ‘woman’, ‘child’로 구분한 범주형 변수.
# 나이와 성별을 함께 고려하여 어린이인지 성인 남성/여성인지 표시합니다.

# deck
#
# 객실이 위치한 갑판(Deck) 정보(A, B, C, D, E, F, G, T 등).
# 데이터가 누락된 경우(NaN)도 많습니다.

# embark_town
# 실제 도시 이름(‘Cherbourg’, ‘Queenstown’, ‘Southampton’)을 표시합니다.

# alone
#
# 혼자 탑승했는지(True/False) 여부를 나타냅니다.
# sibsp + parch가 0이면 True, 그렇지 않으면 False

# 데이터 수집
import seaborn as sns
import pandas as pd
titanic = sns.load_dataset("titanic")
print(titanic.head())
titanic.to_csv("./titanic_2.csv", index=False)
#%%
# 데이터 준비
# 비어 있는 컬럼들 조사 (결측값, 누락된값):
# age, embarked, deck , embark_town
# 정확한 분석이 어렵기 때문에, 대체 값으로 채울 예정.
# age -> 중앙값으로 값을 대체, embarked, deck , embark_town : 최빈값()으로 대체 하자.
# 임의로 정했음. (룰을 임의로 정함, 데이터가 부족하거나, 표본 집단을 정하는 경우도 많음.)
# 최빈값 : 예) 카테고리  suv(70개) , 승용차(100개) , 전기차(30개) -> 승용차 최빈값.

# titanic , 메모리 상에 있는 데이터를 기본 조사, 널 조사.
titanic.isnull().sum()
titanic.info()
#%%
# age, embarked, deck , embark_town
# 정확한 분석이 어렵기 때문에, 대체 값으로 채울 예정.
# age -> 중앙값으로 값을 대체,
# titanic['age'].median() : 중앙값
# fillna 함수 이용해서, null 값에 , 해당 인자값 , 중앙값으로 채우기.
# 다신, age 컬럼에 비어있는 값에, 중앙값으로 채우기.
titanic['age'] = titanic['age'].fillna(titanic['age'].median())
# 결과 확인.
titanic.isnull().sum()

# embarked, deck , embark_town : 최빈값()으로 대체 하자.
# embarked 컬럼의 최빈값 조사 -> S    644 , 사용
embarked = titanic["embarked"].value_counts()
print(f"embarked : \n {embarked}")

# deck -> C    747 ,당첨
deck = titanic["deck"].value_counts()
print(f"deck : \n {deck}")

# # embark_town -> Southampton    644, 당첨
embark_town = titanic["embark_town"].value_counts()
print(f"embark_town : \n {embark_town}")
#
# # 조사 후 , 해당 값으로 , 컬럼의 빈값에, 대체 하기.
titanic["embarked"] = titanic["embarked"].fillna("S")
titanic["deck"] = titanic["deck"].fillna("C")
titanic["embark_town"] = titanic["embark_town"].fillna("Southampton")
#
# # 다시, 결측값 조사.
# # 결과 확인.
titanic.isnull().sum()


#%%
# 데이터 탐색.
# info 함수 이용해서, 기본 정보 확인.
titanic.info()
#%%
# 종속 변수 survived, 생존자 수를 확인.
# 답이 있어요. 지도 학습.
# 사망자: 549명, 생존자: 342명.
titanic.survived.value_counts()
#%%
# 기본적인 차트 그리기, 시각화 하기.
import matplotlib.pyplot as plt
f, ax = plt.subplots(1,2, figsize= (10,5))

# 시각화 속성 옵션 설정하기.
# titanic['survived']
#
# 타이타닉 데이터프레임에서 ‘survived’ 열(시리즈)만 추출합니다.
# 이 열은 탑승객의 생존 여부(0: 사망, 1: 생존)를 담고 있습니다.

# [titanic["sex"] =='male']
#
# ‘sex’ 열에서 값이 ‘male’인 행만 필터링합니다.
# 즉, 남성 승객들의 ‘survived’ 데이터만 선택하게 됩니다.

# .value_counts()
#
# 남성 승객의 ‘survived’ 값(0 또는 1)이 각각 몇 명인지 빈도수를 계산합니다.
# 예: 0 468 / 1 109 형태로 생존/사망자 수를 세어 반환합니다.

# .plot.pie(...)
#
# 계산된 빈도수를 이용해 파이 차트를 그립니다.
# 파라미터:
# explode = [0, 0.1]:
# 파이 차트의 각 조각을 중심에서 얼마나 떨어뜨릴지 결정합니다.
# 첫 번째 조각(0)은 그대로 두고, 두 번째 조각(0.1)은 약간 분리되어 표시됩니다.

# autopct = "%1.1f%%":
# 파이 차트 조각에 표시되는 백분율의 형식을 지정합니다. 여기서는 소수점 첫째 자리까지 표현됩니다. (예: 23.4%)

# ax = ax[0]:
# 미리 생성된 서브플롯 배열(ax)의 첫 번째 축(ax[0])에 파이 차트를 그립니다.
# 여러 그래프를 한 Figure에 그릴 때 사용합니다.

# shadow = True:
# 파이 차트에 그림자를 추가하여 시각적 입체감을 줍니다.
titanic['survived'][titanic["sex"] =='male'].value_counts().plot.pie(explode = [0,0.1], autopct = "%1.1f%%", ax = ax[0], shadow = True)

# 시각화 속성 옵션 설정하기.
titanic['survived'][titanic["sex"] =='female'].value_counts().plot.pie(explode = [0,0.1], autopct = "%1.1f%%", ax = ax[1], shadow = True)

# 제목
ax[0].set_title("survived (male)")
ax[1].set_title("survived (female)")

plt.show()


#%%
# 객실 등급별 생존자수 차트 그리기. hue = 속성, 종속변수
sns.countplot( x= "pclass", hue="survived", data=titanic)
plt.title("P class Survived")
plt.show()
#%%
# 데이터 모델링
# 상관 분석은 , pandas 에서 제공하는 corr() 함수 이용하고,
# 상관 계수 : 피어슨 상관계수를 이용.
# titanic.info()
# 문제점, 해당 데이터 프레임에 숫자가 아닌 문자열이 포함이 되어서, 변환을 못한다는 에러
# 확인 시에는 알아서, 해당 숫자 컬럼만 선택을 했었는데, 다시, 기존 코드부분 확인하니.
# 되었던 코드도 안되는 상황이라서,
# chat gpt에게, corr 함수 속성에서, 숫자 부분만 선택하는 부분을 질문에 답을 받아서, 수정.

# titanic_corr = titanic.corr(method = 'pearson')
# 해결책 -> 해당 데이터 프레임에서, 숫자 부분 컬럼만 가져오기.
import numpy as np
numeric_titanic = titanic.select_dtypes(include=[np.number])
# 숫자 필드로만 구성된 데이터 프레임 확인.
# numeric_titanic
titanic_corr = numeric_titanic.corr(method = 'pearson')
titanic_corr

# csv 파일로 변환
titanic_corr.to_csv("./titanic_corr.csv", index=False)

#%%
# 특정 변수의 상관관계 분석 해보기. adult_male
adult_male_corr = titanic["survived"].corr(titanic["adult_male"])
print(f"adult_male_corr : \n {adult_male_corr}")
# 특정 변수의 상관관계 분석 해보기. fare
fare_corr = titanic["survived"].corr(titanic["fare"])
print(f"fare_corr : \n {fare_corr}")
#%%
# 시각화 해보기.
# seaborn 패키지의 pairplot() 이용해서 시각화
# pairplot -> 그리드(표) 형태로 각 데이터 열의 조합을 산점도(scatter plot)로 그린다.
# 같은 데이터가 만나는 대각선 부분은 히스토 그램으로 그리기.

sns.pairplot(titanic, hue="survived")
plt.show()
#%%
# 두 개의 변수의 상관관계 차트 그리기.
sns.catplot(x="pclass", y="survived", hue= "sex", data=titanic, kind="point")
plt.show()
#%%
# 히트 맵으로 시각화하기.
# 각 변수들에 대해서, 상관 관계 계수를, 해당 영역에 온도로 표시를 함.
# 관련이 높을 수록, 뜨거운 온도를 빚대어서, 빨간색으로 표시, 음의 상관 관계이면, 파란색으로

# 변수 중에 age : 나이 구간별로, 카테고리 작업해서, 시각화를 용이하게 할려고.
# 10 살 미만 : 카테고리 0
# 10 살 이상 ~ 20살 미만 : 카테고리 1
# 20 살 이상 ~ 30살 미만 : 카테고리 2
# 30 살 이상 ~ 40살 미만 : 카테고리 3
# 40 살 이상 ~ 50살 미만 : 카테고리 4
# 50 살 이상 ~ 60살 미만 : 카테고리 5
# 60 살 이상 ~ 70살 미만 : 카테고리 6
# 80 살 이상 ~ : 카테고리 7

def catogoryAge(x):
  if x < 10:
    return 0
  elif x <20:
    return 1
  elif x <30:
    return 2
  elif x <40:
    return 3
  elif x <50:
    return 4
  elif x <60:
    return 5
  elif x <70:
    return 6
  else:
    return 7

# 새로운 카테고리 컬럼 categoryAge => age2
titanic['age2'] = titanic["age"].apply(catogoryAge)

# 성비도 문자열에서, 남자 : 1, 여자 : 0
titanic["sex"] = titanic["sex"].map({"male" : 1, "female" : 0})

# 가족의 수를 따로 컬럼을 추가하기. family
# sibsp : 형제 자매, 배우자 , parch: 부모, 어린이
titanic["family"] = titanic["sibsp"] + titanic["parch"] + 1

# 새로운 데이터를 csv 파일로 변환
titanic.to_csv("./titanic_new_family.csv", index=False)
#%%
# 데이터를 새로 변경 후, 히트맵 그리기
heatmap_data = titanic[["survived", "sex", "age2", "family", "pclass","fare"]]
print(heatmap_data.head())
colormap = plt.cm.RdBu

# heatmap_data.astype(float):
# 데이터의 타입을 float로 변환합니다. (문자열 등 다른 타입이 섞여 있을 경우 문제 발생을 방지하기 위함)
# .corr():
# 변환된 데이터에 대해 상관계수 행렬을 계산합니다.

# linewidths=0.1
# 각 셀(상관계수 값이 들어있는 사각형) 사이에 0.1 포인트 두께의 선을 그립니다.
#
# vmax=1.0
# 색상 스케일의 최대값을 1.0으로 고정합니다. 상관계수의 최대 값인 1에 해당하는 색상을 지정합니다.
#
# square=True
# 각 셀을 정사각형 모양으로 만듭니다. (세로와 가로 길이를 동일하게)
#
# cmap=colormap
# 사용자가 미리 정의한 colormap(색상 팔레트)을 적용합니다.
# (예: 'coolwarm', 'viridis' 등과 같이 색상 팔레트를 지정할 수 있습니다.)
#
# linecolor="white"
# 셀 경계선을 흰색으로 설정합니다.
#
# annot=True
# 각 셀 안에 상관계수 값을 텍스트로 표시합니다.
#
# annot_kws={"size": 10}
# 텍스트(상관계수) 표시 시 폰트 크기를 10으로 지정합니다.
sns.heatmap(heatmap_data.astype(float).corr(), linewidths=0.1, vmax= 1.0 , square=True, cmap = colormap, linecolor="white", annot = True, annot_kws={"size": 10})
# 해석
# survived(생존 여부)와 sex(성별) 간 상관(-0.54)
# 음의 상관계수가 크다는 것은, 성별이 ‘남성(1)’일수록 생존(1)과 반대로 움직이는 경향이 강함을 의미합니다.
# 즉, 남성보다는 여성이 생존 확률이 높았음을 시사합니다

# survived와 pclass(티켓 등급) 간 상관(-0.34)
# 등급 숫자가 높을수록(3등급) 생존율이 낮아지는 경향을 나타냅니다.
# 즉, 1등급에 가까울수록(숫자가 작을수록) 생존 가능성이 커집니다.
#%%
--

설명 예시1 :

---

### **Ch1-2 코드 종합 설명**

이 코드는 `seaborn` 라이브러리에 내장된 **타이타닉호 탑승객 데이터**를 사용하여 **생존 여부(`survived`)**와 다른 특성들(성별, 나이, 객실 등급 등) 간의 관계를 분석합니다. 주요 목표는 어떤 요인이 생존에 큰 영향을 미쳤는지 파악하는 것입니다.

1.  **데이터 준비 및 전처리:** 데이터를 불러온 뒤, 분석에 방해가 되는 **결측치(Null 값)**를 찾아냅니다. `age`는 **중앙값**으로, `embarked`와 `deck` 등은 **최빈값**(가장 빈번하게 나타나는 값)으로 채워 넣어 데이터를 정제합니다.
2.  **탐색적 데이터 분석 (EDA):** `matplotlib`과 `seaborn`을 이용해 **파이 차트**와 **카운트 플롯**을 그려 성별, 객실 등급에 따른 생존율의 차이를 시각적으로 탐색합니다.
3.  **상관 분석 및 피처 엔지니어링:** `pandas`의 `corr()` 함수를 사용해 모든 숫자형 변수 간의 **피어슨 상관계수**를 계산합니다. 또한, 분석의 정확도를 높이기 위해 기존 변수들을 가공하여 `age2`(나이대), `family`(가족 수)와 같은 새로운 **파생 변수**를 만드는 **피처 엔지니어링**을 수행합니다.
4.  **시각화 (히트맵):** 최종적으로 계산된 상관계수 행렬을 **히트맵**으로 시각화하여, 어떤 변수들이 생존 여부와 강한 양 또는 음의 관계를 가지는지 한눈에 파악하고 결론을 도출합니다.

---

### **Part 1: 데이터 수집 및 전처리 (결측치 처리)**

어떤 분석이든 가장 먼저 수행해야 할, 데이터를 깨끗하게 만드는 과정입니다.

#### **1. 코드, 문법 및 개별 설명**

```python
# %%
import seaborn as sns
import pandas as pd

# Seaborn에 내장된 'titanic' 데이터셋을 불러옴
titanic = sns.load_dataset("titanic")

# 데이터의 결측치(null) 개수를 컬럼별로 확인
titanic.isnull().sum()
# 결과값 (예시):
# age            177  -> 177개의 나이 정보가 비어있음
# embarked         2
# deck           688  -> 688개의 deck 정보가 비어있음
# embark_town      2
# ... (다른 컬럼은 0)

# 'age' 컬럼의 결측치를 'age'의 중앙값으로 채움
# 연산 과정:
# 1. titanic['age'].median()으로 나이의 중앙값(예: 28.0)을 계산
# 2. .fillna() 함수가 'age' 컬럼에서 null인 값들을 찾아 28.0으로 대체
titanic['age'] = titanic['age'].fillna(titanic['age'].median())

# 'embarked', 'deck', 'embark_town' 컬럼의 최빈값을 확인
# .value_counts()는 각 값의 개수를 세어 내림차순으로 보여줌
print(titanic["embarked"].value_counts())  # 결과: S가 644로 가장 많음
print(titanic["deck"].value_counts())      # 결과: C가 59로 가장 많음 (주석과 달리 C가 최빈값)
print(titanic["embark_town"].value_counts()) # 결과: Southampton이 644로 가장 많음

# 각 컬럼의 결측치를 해당 컬럼의 최빈값으로 채움
titanic["embarked"] = titanic["embarked"].fillna("S")
titanic["deck"] = titanic["deck"].fillna("C") # 'C'가 실제 데이터의 최빈값
titanic["embark_town"] = titanic["embark_town"].fillna("Southampton")

# 모든 결측치가 처리되었는지 다시 확인
titanic.isnull().sum()
# 결과: 모든 컬럼의 결측치 개수가 0으로 표시됨
```

#### **2. 해당 설명**

"Garbage in, garbage out" (쓰레기를 넣으면 쓰레기가 나온다)는 데이터 분석의 유명한 격언입니다. 분석의 신뢰도는 데이터의 품질에 달려있기 때문에, **결측치 처리**는 매우 중요한 첫 단계입니다. 위 코드에서는 두 가지 대표적인 결측치 대체 방법을 사용했습니다.

*   **중앙값(Median) 대체:** `age`와 같은 숫자형 데이터에 사용됩니다. 평균(mean)은 극단적인 값(아주 늙거나 어린 나이)에 영향을 많이 받지만, 중앙값은 상대적으로 안정적이라 이상치(outlier)가 있을 때 더 좋은 선택입니다.
*   **최빈값(Mode) 대체:** `embarked`(탑승 항구)와 같은 범주형 데이터에 사용됩니다. 가장 많이 등장한 값으로 채우는 것이 가장 합리적인 추정이기 때문입니다.

#### **3. 응용 가능한 예제**

**"온라인 쇼핑몰 고객 데이터의 누락된 '가입 경로' 정보 채우기"**

고객 데이터에 '가입 경로'(예: 'Google 검색', '인스타그램 광고', '지인 추천') 정보가 일부 누락되었을 때, 전체 데이터에서 가장 빈번한 가입 경로(최빈값)를 찾아 누락된 값을 채워넣어 마케팅 분석의 정확도를 높일 수 있습니다.

#### **4. 추가하고 싶은 내용 (결측치 삭제)**

결측치를 채우는 대신, 해당 행 전체를 삭제하는 방법(`dropna()`)도 있습니다. 이는 결측치가 있는 데이터의 양이 전체에 비해 매우 적을 때(예: 1% 미만) 간단하게 사용할 수 있습니다. 하지만 `deck` 컬럼처럼 결측치가 너무 많을(688/891, 약 77%) 경우, 해당 컬럼 자체를 분석에서 제외하는 것(`drop()`)이 더 나은 판단일 수 있습니다.

#### **5. 심화 내용 (결측치 자체의 의미)**

때로는 데이터가 **'왜 비어있는가'** 자체가 중요한 정보일 수 있습니다. 예를 들어, 타이타닉 데이터에서 `deck` 정보가 없는 사람들은 애초에 객실을 배정받지 못한 하층민일 가능성이 높습니다. 따라서 무작정 최빈값으로 채우기보다는, 'Deck_Unknown'이라는 새로운 카테고리를 만들거나, 결측치 여부 자체를 하나의 변수(예: `has_deck_info` = 0 또는 1)로 만들어 분석에 활용하는 고급 기법도 존재합니다.

---

### **Part 2: 탐색적 데이터 분석(EDA) 및 기본 시각화**

데이터를 정제한 후, 본격적인 분석에 앞서 데이터의 특징을 시각적으로 살펴보는 과정입니다.

#### **1. 코드, 문법 및 개별 설명**

```python
# %%
import matplotlib.pyplot as plt
import seaborn as sns

# 1행 2열의 서브플롯(그래프 그릴 공간)을 생성. 전체 크기는 가로 10, 세로 5
f, ax = plt.subplots(1, 2, figsize=(10, 5))

# 남성 생존율 파이 차트 (왼쪽)
# 연산 과정:
# 1. titanic["sex"] == 'male' : 성별이 남성인 행들만 필터링
# 2. titanic['survived'][...] : 필터링된 행들에서 'survived' 열만 선택
# 3. .value_counts() : 남성의 생존(1)/사망(0)자 수를 각각 계산
# 4. .plot.pie(...) : 계산된 값을 파이 차트로 그림. autopct는 백분율 표시 형식, ax=ax[0]는 왼쪽 공간에 그리라는 의미
titanic['survived'][titanic["sex"] == 'male'].value_counts().plot.pie(explode=[0, 0.1], autopct="%1.1f%%", ax=ax[0], shadow=True)

# 여성 생존율 파이 차트 (오른쪽)
titanic['survived'][titanic["sex"] == 'female'].value_counts().plot.pie(explode=[0, 0.1], autopct="%1.1f%%", ax=ax[1], shadow=True)

# 각 차트의 제목 설정
ax[0].set_title("Survived (male)")
ax[1].set_title("Survived (female)")
plt.show()

# 객실 등급(pclass)별 생존자/사망자 수를 막대그래프로 시각화
# 연산 과정:
# 1. x="pclass": x축을 객실 등급으로 설정
# 2. hue="survived": 'survived' 값(0 또는 1)에 따라 막대 색을 다르게 표현
# 3. data=titanic: 사용할 데이터프레임을 지정
sns.countplot(x="pclass", hue="survived", data=titanic)
plt.title("Pclass vs Survived")
plt.show()
# 결과 해석:
# 파이 차트 -> 여성의 생존율(약 74.2%)이 남성(약 18.9%)보다 압도적으로 높음을 확인.
# 카운트 플롯 -> 1등급 객실은 생존자 수가 사망자 수보다 많지만, 3등급 객실은 사망자 수가 압도적으로 많음을 확인.
```

#### **2. 해당 설명**

이 파트에서는 **성별**과 **객실 등급**이 생존율에 큰 영향을 미쳤을 것이라는 가설을 세우고, 이를 시각적으로 빠르게 확인합니다. **파이 차트**는 전체 대비 각 부분의 비율을 보여주는 데 효과적이며, 이를 통해 남성과 여성의 생존율이 극명하게 갈렸음을 즉시 알 수 있습니다. **카운트 플롯(`countplot`)**은 범주형 데이터의 개수를 세어 막대그래프로 보여주는 `seaborn`의 유용한 함수로, 객실 등급이 낮아질수록 사망자 수가 급격히 늘어나는 패턴을 명확하게 보여줍니다.

#### **3. 응용 가능한 예제**

**"요일 및 시간대별 카페 메뉴 판매량 분석"**

`countplot`을 이용해 x축을 '요일', `hue`를 '메뉴 카테고리(커피/논커피)'로 설정하여 어떤 요일에 어떤 종류의 음료가 많이 팔리는지 시각화하고, 이를 통해 재고 관리 및 프로모션 전략을 수립할 수 있습니다.

#### **4. 추가하고 싶은 내용 (Box Plot 활용)**

`countplot` 외에도 `boxplot`이나 `violinplot`을 사용하면 범주에 따른 숫자 데이터의 분포를 더 상세하게 볼 수 있습니다. 예를 들어, `sns.boxplot(x='survived', y='age', data=titanic)` 코드를 실행하면 생존자와 사망자의 나이 분포(중앙값, 사분위수, 이상치 등)를 비교하여 '어린 아이들이 더 많이 살았는가?'와 같은 질문을 탐색할 수 있습니다.

#### **5. 심화 내용 (통계적 유의성 검증)**

시각화를 통해 발견한 차이가 우연인지 통계적으로 의미가 있는 것인지 확인하려면 **카이제곱 검정(Chi-squared test)**을 사용할 수 있습니다. `성별`과 `생존여부` 같은 두 범주형 변수 간의 독립성(관련이 있는지 없는지)을 검증하는 데 사용되며, "성별과 생존율은 통계적으로 유의미한 관계가 있다"는 결론을 수치적으로 뒷받침할 수 있습니다.

---

### **Part 3: 상관 분석 및 피처 엔지니어링**

데이터 간의 수치적 관계를 정량화하고, 더 나은 분석을 위해 새로운 특징(Feature)을 만들어내는 과정입니다.

#### **1. 코드, 문법 및 개별 설명**

```python
# %%
import numpy as np

# 상관계수 계산을 위해 데이터프레임에서 숫자형(numeric) 컬럼만 선택
numeric_titanic = titanic.select_dtypes(include=[np.number])

# 숫자형 데이터 간의 피어슨 상관계수 행렬을 계산
titanic_corr = numeric_titanic.corr(method='pearson')
print(titanic_corr)

# 'survived'와 'adult_male' 사이의 상관계수만 별도로 계산
# adult_male은 True/False 값을 가지므로, 계산 시 1/0으로 자동 변환됨
adult_male_corr = titanic["survived"].corr(titanic["adult_male"])
# 결과: -0.557728 -> 성인 남성일수록 생존율이 낮아지는 강한 음의 상관관계

# 'survived'와 'fare'(요금) 사이의 상관계수 계산
fare_corr = titanic["survived"].corr(titanic["fare"])
# 결과: 0.257307 -> 요금을 많이 낼수록 생존율이 높아지는 약한 양의 상관관계

# -- 피처 엔지니어링 --
# 나이를 구간별 카테고리로 변환하는 함수 정의
def catogoryAge(x):
    # ... (함수 내용 생략) ...
    return category

# apply 함수를 이용해 'age' 컬럼의 모든 값에 catogoryAge 함수를 적용, 결과를 'age2' 새 컬럼에 저장
titanic['age2'] = titanic["age"].apply(catogoryAge)

# 'sex' 컬럼의 문자열('male'/'female')을 숫자(1/0)로 변환
titanic["sex"] = titanic["sex"].map({"male": 1, "female": 0})

# 'sibsp'(형제/배우자)와 'parch'(부모/자녀)를 더하고 1(본인)을 추가하여 'family'(총 가족 수) 컬럼 생성
titanic["family"] = titanic["sibsp"] + titanic["parch"] + 1

# -- 히트맵 시각화 --
# 히트맵에 사용할 주요 변수들만 선택하여 새로운 데이터프레임 생성
heatmap_data = titanic[["survived", "sex", "age2", "family", "pclass", "fare"]]

colormap = plt.cm.RdBu # 색상 맵 설정 (Red-Blue)
plt.figure(figsize=(10, 8))

# 상관계수 행렬을 히트맵으로 시각화
# 연산 과정:
# 1. heatmap_data.astype(float).corr() : 선택된 데이터의 상관계수 행렬을 계산
# 2. sns.heatmap(...) : 계산된 행렬을 기반으로 히트맵을 그림
#    - annot=True : 각 셀에 상관계수 값을 표시
#    - cmap=colormap : 빨간색(양의 상관) ~ 파란색(음의 상관)으로 색상을 표현
#    - vmax=1.0 : 색상 스케일의 최대값을 1.0으로 고정
sns.heatmap(heatmap_data.astype(float).corr(), linewidths=0.1, vmax=1.0, square=True, cmap=colormap, linecolor="white", annot=True, annot_kws={"size": 10})
plt.show()
```

#### **2. 해당 설명**

이 파트는 분석의 핵심입니다. **상관 분석**은 두 변수 간의 선형적 관계의 강도와 방향을 -1에서 1 사이의 숫자로 나타냅니다. `pandas`의 `.corr()` 함수는 이를 쉽게 계산해줍니다.

**피처 엔지니어링**은 모델의 성능을 높이기 위해 데이터를 가공하는 창의적인 과정입니다.
*   **Binning (구간화):** 연속형 변수인 `age`를 `age2`라는 범주형 변수로 만들어, 단순한 나이보다 '나이대'(유아, 청소년, 성인 등)가 생존에 미치는 영향을 더 명확하게 볼 수 있도록 합니다.
*   **Mapping (매핑):** `sex`와 같은 문자열 데이터를 모델이 이해할 수 있는 숫자(0, 1)로 변환합니다.
*   **Feature Creation (파생 변수 생성):** `sibsp`와 `parch`를 합쳐 `family`라는 새로운 변수를 만듦으로써, '가족의 규모'라는 더 의미 있는 정보를 추출합니다.

마지막으로 **히트맵**은 이렇게 계산된 복잡한 상관계수 행렬을 색깔로 표현하여, 어떤 변수들이 서로 강한 관계를 맺고 있는지 직관적으로 파악하게 해주는 최고의 시각화 도구입니다.

#### **3. 응용 가능한 예제**

**"고객 이탈(Churn) 예측 모델을 위한 피처 엔지니어링"**

'가입일'과 '마지막 접속일'이라는 날짜 데이터로부터 '총 서비스 이용 기간'이라는 파생 변수를 만들고, '월 평균 구매 횟수'를 구간화하여 '충성 고객', '일반 고객', '비활성 고객'으로 나누는 등, 새로운 피처를 만들어 고객 이탈 예측 모델의 성능을 향상시킬 수 있습니다.

#### **4. 추가하고 싶은 내용 ("Correlation is not Causation")**

분석에서 가장 중요한 격언 중 하나는 **"상관관계는 인과관계가 아니다"** 입니다. 히트맵에서 `fare`(요금)와 `survived`(생존)가 양의 상관관계를 보인다고 해서 "돈을 많이 낸 것이 생존의 원인이다"라고 단정할 수 없습니다. 실제로는 요금이 비싼 `pclass` 1등급 객실이 구명보트와 가까운 곳에 위치했기 때문일 수 있습니다. 즉, `pclass`라는 제3의 변수가 둘 모두에 영향을 미친 것입니다. 상관 분석은 관계를 찾는 출발점일 뿐, 원인을 단정하는 도구는 아님을 항상 명심해야 합니다.

#### **5. 심화 내용 (다중 공선성 문제)**

히트맵은 종속 변수(`survived`)와 독립 변수들 간의 관계를 보는 데도 유용하지만, **독립 변수들끼리의 관계**를 파악하는 데도 매우 중요합니다. 만약 서로 다른 독립 변수 두 개가 매우 높은 상관관계(예: 0.9 이상)를 보인다면, 이는 **다중 공선성(Multicollinearity)** 문제가 있다는 신호입니다. 이는 회귀 모델의 안정성을 해치므로, 두 변수 중 하나를 제거하거나 주성분 분석(PCA) 같은 차원 축소 기법을 고려해야 합니다. 타이타닉 데이터에서는 `pclass`와 `fare`가 강한 음의 상관(-0.55)을 보이는데, 이는 자연스러운 현상입니다(등급이 높을수록(숫자는 작아짐) 요금은 비싸짐).

--
예시코드 2 :

# 시각화에 필요한 패키지 임포트 하기.
import matplotlib.pyplot as plt
import seaborn as sns

import matplotlib.font_manager as fm
# 폰트 경로 설정
# font_path = "C:/Windows/Fonts/NanumGothic.ttf"
font_path= "C:/Windows/Fonts/malgun.ttf"  # '맑은 고딕' 폰트 경로
font_prop = fm.FontProperties(fname=font_path)

# Matplotlib 기본 폰트 설정
plt.rc('font', family=font_prop.get_name())

# 음수 기호 깨짐 방지
plt.rcParams['axes.unicode_minus'] = False

sns.set_style("dark")
# red wine 속성 설정.
# sns.distplot:
# 데이터의 분포(히스토그램)와 함께 커널 밀도 추정(kde)을 시각화하는 함수입니다.
# red_wine_quality:
# 빨간 와인의 품질 데이터를 담은 배열이나 시리즈입니다.
# kde=True:
# 히스토그램과 함께 부드러운 밀도 곡선(kde)을 그려, 데이터 분포의 연속적인 형태를 보여줍니다.
# color="red":
# 그래프의 색상을 빨간색으로 지정하여, red wine 데이터를 시각적으로 구분합니다.
# label=" red wine":
# 범례에 표시할 라벨로, 그래프에서 해당 데이터가 빨간 와인임을 나타냅니다.
sns.distplot(red_wine_quality, kde=True, color="red", label =" red wine")

# white wine 속성 설정.
sns.distplot(white_wine_quality, kde=True, label =" white wine")

plt.title("와인 타입에 따른 품질 " ,fontproperties=font_prop)
plt.legend()
plt.show()

--
설명 예시 2:

---

### **Ch1 코드 종합 설명**

이 코드는 레드 와인과 화이트 와인의 화학적 특성 데이터를 사용하여 와인의 **품질(quality)을 분석하고 예측**하는 과정을 담고 있습니다. 전체적인 흐름은 다음과 같습니다.

1.  **데이터 준비:** 두 개의 개별 CSV 파일(`winequality-red.csv`, `winequality-white.csv`)을 `pandas` 라이브러리를 이용해 읽어오고, 두 데이터를 하나로 병합하여 분석을 위한 통합 데이터셋을 만듭니다.
2.  **데이터 탐색 및 분석:** 통합된 데이터의 기본 정보를 확인하고(기술 통계), `t-검정`을 통해 레드 와인과 화이트 와인 그룹 간 품질에 통계적으로 유의미한 차이가 있는지 검증합니다.
3.  **회귀 모델링:** `statsmodels` 라이브러리를 사용하여 와인의 여러 화학적 특성(독립 변수)들이 품질(종속 변수)에 어떤 영향을 미치는지 설명하는 **선형 회귀 모델**을 만듭니다.
4.  **예측 및 시각화:** 생성된 회귀 모델을 사용해 실제 데이터와 임의의 데이터에 대한 품질 등급을 예측해봅니다. 마지막으로 `seaborn`과 `matplotlib`을 이용해 분석 결과를 **히스토그램**과 **산점도**로 시각화하여 직관적인 인사이트를 도출합니다.

---

### **Part 1: 데이터 준비 및 전처리**

분석에 앞서 흩어져 있는 데이터를 불러오고, 분석하기 좋은 형태로 가공하는 과정입니다.

#### **1. 코드, 문법 및 개별 설명**

```python
# %%
# 데이터 준비,
# 파일 정리 간단히 하기.
# ... (주석 생략) ...
import pandas as pd

# 레드와인 데이터를 세미콜론(;)을 기준으로 분리하여 DataFrame으로 읽어옴
red_df = pd.read_csv("./winequality-red.csv",
                     sep=";", header=0, engine='python')

# 화이트와인 데이터를 세미콜론(;)을 기준으로 분리하여 DataFrame으로 읽어옴
white_df = pd.read_csv("./winequality-white.csv",
                       sep=";", header=0, engine='python')

# 각 DataFrame에 와인 종류를 구분할 'type' 열을 맨 앞에 추가
# 연산 과정:
# 1. red_df의 0번째 열 위치에 'type'이라는 이름의 열을 생성
# 2. 해당 열의 모든 값을 'red'로 채움
red_df.insert(0, column="type", value="red")
white_df.insert(0, column="type", value="white")

# 두 DataFrame(red_df, white_df)을 위아래로 이어 붙여 하나의 DataFrame으로 합침
# 연산 순서: red_df 아래에 white_df가 그대로 연결됨
wine = pd.concat([red_df, white_df])

# 컬럼(열) 이름에 포함된 공백(" ")을 밑줄("_")로 변경
# 예시: 'fixed acidity' -> 'fixed_acidity'
# 연산 과정:
# 1. wine.columns는 컬럼 이름 리스트를 반환
# 2. .str.replace(" ", "_")는 각 문자열에 대해 치환 작업을 수행
# 3. 변경된 컬럼 이름 리스트를 다시 wine.columns에 할당
wine.columns = wine.columns.str.replace(" ", "_")

# 최종적으로 통합되고 정리된 데이터를 'wine.csv' 파일로 저장
# index=False 옵션은 DataFrame의 인덱스(0, 1, 2...)가 파일에 저장되지 않도록 함
wine.to_csv("./wine.csv", index=False)
```

#### **2. 해당 설명**

데이터 분석의 첫 단추는 데이터를 불러와 정제하는 것입니다. 위 코드는 `pandas` 라이브러리를 핵심적으로 사용합니다. `pd.read_csv` 함수로 원본 데이터를 메모리로 불러오고, `insert` 함수를 통해 각 데이터가 레드 와인인지 화이트 와인인지 구별할 수 있는 **'type'이라는 중요한 분류 기준**을 추가했습니다. `pd.concat`은 이렇게 준비된 두 데이터를 합쳐 단일 데이터셋으로 만드는 역할을 합니다. 마지막으로, 컬럼 이름의 공백을 밑줄로 바꾸는 작업은 사소해 보이지만, 이후 `wine.fixed_acidity`와 같이 코드를 작성할 때 공백으로 인한 오류를 방지하는 매우 유용한 전처리 과정입니다.

#### **3. 응용 가능한 예제**

**"여러 매장의 월별 매출 데이터를 하나의 연간 데이터로 통합하기"**

각 매장별로 `jan_sales.csv`, `feb_sales.csv`, ... 와 같이 파일이 나뉘어 있을 때, 각 파일을 불러와 'month' 열을 추가한 뒤 `pd.concat`으로 합쳐 연간 매출 분석을 위한 통합 데이터를 만들 수 있습니다.

#### **4. 추가하고 싶은 내용 (데이터 타입 최적화)**

데이터를 불러온 후 `wine.info()`를 실행해 보면 모든 숫자형 데이터가 `float64` 또는 `int64`로 되어 있습니다. 만약 데이터가 수백만 건 이상으로 매우 클 경우, 메모리 사용량이 부담될 수 있습니다. 이때 `quality`와 같이 범위가 작은 정수는 `int8`로, 다른 실수형 데이터는 `float32` 등으로 데이터 타입을 변경(`astype` 함수 사용)하면 메모리를 효율적으로 사용할 수 있습니다.

#### **5. 심화 내용 (ETL 파이프라인 개념)**

지금과 같은 작업(데이터 추출, 변환, 적재)을 **ETL(Extract, Transform, Load)**이라고 부릅니다. 실제 대규모 데이터 환경에서는 이러한 과정을 자동화하는 '데이터 파이프라인'을 구축합니다. Apache Airflow, Prefect 같은 도구들은 여러 데이터 소스에서 데이터를 주기적으로 가져와 전처리한 후, 분석용 데이터베이스에 저장하는 복잡한 작업을 자동화하는 데 사용됩니다. 현재의 코드는 이러한 파이프라인의 가장 기본적인 형태라고 할 수 있습니다.

---

### **Part 2: 탐색적 데이터 분석 (EDA) 및 t-검정**

데이터를 깊이 분석하기 전에, 데이터의 기본적인 특성과 분포를 파악하고 통계적 가설을 검증하는 단계입니다.

#### **1. 코드, 문법 및 개별 설명**

```python
# %%
# ... (중략) ...

# 데이터의 전체적인 구조와 정보(총 행 수, 각 열의 데이터 타입, 결측치 여부)를 출력
print(wine.info())
# 결과값 예시:
# <class 'pandas.core.frame.DataFrame'>
# RangeIndex: 6497 entries, 0 to 4897
# Data columns (total 13 columns):
# ...
# dtypes: float64(11), int64(1), object(1)
# memory usage: 682.7+ KB

# 숫자형 데이터에 대한 주요 기술 통계량(개수, 평균, 표준편차, 최소값, 사분위수, 최대값)을 요약하여 보여줌
wine.describe()

# 'quality' 열의 값별로 개수를 세어 보여줌 (어떤 품질 등급이 가장 많은지 확인)
wine.quality.value_counts()
# 결과값 예시:
# quality
# 6    2836
# 5    2138
# 7    1079
# ...

# 와인 종류('type')에 따라 그룹을 나누고, 각 그룹의 'quality' 열에 대한 기술 통계량을 계산
wine.groupby("type")["quality"].describe()
# 연산 과정:
# 1. 'type' 열의 값('red', 'white')에 따라 데이터를 두 그룹으로 나눔
# 2. 각 그룹에서 'quality' 열만 선택
# 3. 각 'quality' 열에 대해 .describe() 함수를 적용하여 통계량을 계산

# t-검정을 위해 레드 와인과 화이트 와인의 quality 데이터만 각각 추출
red_wine_quality = wine.loc[wine['type'] == "red", "quality"]
white_wine_quality = wine.loc[wine['type'] == "white", "quality"]

from scipy import stats

# 두 그룹(레드/화이트 와인 품질)의 평균이 통계적으로 유의미하게 다른지 t-검정 수행
# equal_var=False : 두 그룹의 분산이 다르다고 가정 (Welch's t-test)
stats.ttest_ind(red_wine_quality, white_wine_quality, equal_var=False)
# 결과값 예시: TtestResult(statistic=-10.149363059143164, pvalue=8.168348870049682e-24)
# 연산 과정:
# 1. 두 그룹의 평균, 표준편차, 샘플 크기를 계산
# 2. 이를 이용해 t-통계량(statistic)을 계산
# 3. t-통계량을 바탕으로 p-value(두 그룹의 평균이 실제로는 같은데 우연히 이 정도의 차이가 관찰될 확률)를 계산
# p-value가 매우 작으므로(e-24는 10의 -24제곱), 두 그룹의 품질 평균 차이는 우연이 아니라고 결론 내릴 수 있음
```

#### **2. 해당 설명**

**탐색적 데이터 분석(EDA)**은 데이터에 숨겨진 패턴이나 이상치를 발견하고, 분석 방향을 설정하는 중요한 과정입니다. `info()`, `describe()`, `value_counts()`는 EDA의 가장 기본적인 도구입니다. `groupby`는 데이터를 특정 기준으로 쪼개어 비교 분석할 때 매우 강력한 기능입니다. 이 코드에서는 `t-검정`을 통해 '레드 와인과 화이트 와인의 평균 품질은 다르다'는 가설을 통계적으로 검증했습니다. **p-value**가 0.05보다 현저히 작게 나왔으므로, 두 와인 그룹 간의 품질 차이는 통계적으로 매우 유의미하다고 해석할 수 있습니다.

#### **3. 응용 가능한 예제**

**"A/B 테스트 결과 분석"**

웹사이트의 버튼 색상을 바꾼 A안과 B안을 사용자 그룹에게 노출시킨 후, 각 그룹의 클릭률(Conversion Rate) 데이터를 수집합니다. 그 후 `ttest_ind`를 사용해 두 그룹의 평균 클릭률 차이가 통계적으로 유의미한지 검증하여 어떤 디자인이 더 효과적인지 판단할 수 있습니다.

#### **4. 추가하고 싶은 내용 (피벗 테이블 활용)**

`groupby`와 유사하지만, 결과를 마치 엑셀의 피벗 테이블처럼 행과 열로 재구성하여 보여주는 `pivot_table` 함수도 매우 유용합니다. 예를 들어, 와인 종류별, 품질 등급별 알코올 도수의 평균을 한눈에 보고 싶을 때 사용할 수 있습니다.

```python
pd.pivot_table(wine, values='alcohol', index='type', columns='quality', aggfunc='mean')
```

#### **5. 심화 내용 (통계적 가설 검정의 깊은 이해)**

t-검정 외에도 데이터의 특성에 따라 다양한 검정 방법이 존재합니다. 예를 들어, 세 개 이상의 그룹 평균을 비교할 때는 **분산 분석(ANOVA)**을 사용하고, 범주형 데이터 간의 관련성을 볼 때는 **카이제곱 검정(Chi-squared test)**을 사용합니다. 분석의 목적과 데이터의 종류에 맞는 올바른 통계적 검정 방법을 선택하는 능력을 기르는 것이 중요합니다.

---

### **Part 3: 선형 회귀 분석 및 예측**

데이터 간의 관계를 수학적 모델로 설명하고, 이를 통해 미래의 값을 예측하는 단계입니다.

#### **1. 코드, 문법 및 개별 설명**

```python
# %%
# ... (중략) ...
from statsmodels.formula.api import ols

# 회귀 분석을 위한 수식(formula) 정의
# quality를 종속 변수로, 나머지 화학적 특성들을 독립 변수로 설정
# ~ 기호는 '...에 의해 결정된다'는 의미
Rformula = "quality ~ fixed_acidity + volatile_acidity + citric_acid + residual_sugar + chlorides + free_sulfur_dioxide + total_sulfur_dioxide + density + pH + sulphates + alcohol"

# ols 함수를 이용해 선형 회귀(Ordinary Least Squares) 모델을 생성하고, fit()으로 학습시킴
# 연산 과정:
# 1. ols()는 Rformula와 데이터(wine)를 받아 모델의 구조를 정의
# 2. fit()은 RSS(잔차 제곱합)를 최소화하는 각 독립 변수의 계수(coefficient)를 계산하여 모델을 완성
regression_result = ols(Rformula, data=wine).fit()

# 학습된 회귀 모델의 상세한 요약 정보를 출력
regression_result.summary()
# 결과값: R-squared, Adj. R-squared, 각 변수의 coef(계수), P>|t|(p-value) 등이 포함된 통계표

# 예측에 사용할 샘플 데이터 준비 (기존 데이터에서 처음 5개 행)
# difference 함수를 이용해 'quality'와 'type' 열을 제외한 나머지 열만 선택
sample1 = wine[wine.columns.difference(["quality", "type"])]
sample1_2 = sample1[0:5][:]

# 학습된 회귀 모델을 이용해 샘플 데이터의 quality를 예측
sample1_predict = regression_result.predict(sample1_2)

# 예측 결과와 실제 값을 비교 출력
print(f"예측값: {sample1_predict}")
print(f"실제값: {wine[0:5]['quality']}")
# 예측값: [5.29 5.09 5.14 5.76 5.29] (예시)
# 실제값: [5 5 5 6 5] (예시)
```

#### **2. 해당 설명**

**회귀 분석**은 변수들 사이의 인과관계를 파악하는 데 사용되는 강력한 분석 기법입니다. `statsmodels` 라이브러리의 `ols` 함수는 이를 쉽게 구현하도록 도와줍니다. `Rformula`는 "quality는 다른 화학 성분들에 의해 설명된다"는 모델의 가설을 표현합니다. `fit()`을 통해 학습이 완료된 `regression_result` 객체에는 분석에 대한 모든 정보가 담겨있습니다. `summary()`는 이 결과를 해석하는 데 가장 중요한 함수입니다.

-   **`R-squared`**: 모델이 데이터의 분산을 얼마나 잘 설명하는지를 나타내는 지표 (1에 가까울수록 좋음).
-   **`coef`**: 각 독립 변수가 1단위 증가할 때 종속 변수(quality)가 얼마나 변하는지를 나타냄. (예: `alcohol`의 계수가 0.2라면, 알코올이 1도 오를 때 품질 점수가 0.2점 오르는 경향이 있다는 의미)
-   **`P>|t|`**: 각 변수의 계수가 통계적으로 유의미한지(0에 가까운지)를 나타내는 p-value. 이 값이 0.05보다 작으면 해당 변수는 품질에 유의미한 영향을 미친다고 해석할 수 있습니다.

`predict` 함수는 이렇게 학습된 모델(수식)에 새로운 독립 변수 값들을 대입하여 결과(종속 변수)를 예측하는 기능입니다.

#### **3. 응용 가능한 예제**

**"온라인 광고비 지출에 따른 매출액 예측"**

페이스북, 구글, 인스타그램 등 각 채널에 지출한 광고비(독립 변수)와 그에 따른 일일 매출액(종속 변수) 데이터를 사용하여 회귀 모델을 만들 수 있습니다. 이를 통해 "다음 달 광고 예산을 각 채널에 어떻게 배분해야 매출을 극대화할 수 있을까?"와 같은 질문에 대한 데이터 기반의 답을 찾을 수 있습니다.

#### **4. 추가하고 싶은 내용 (더미 변수 처리)**

현재 모델에는 `type`이라는 범주형 변수가 포함되어 있지 않습니다. 만약 와인 종류(`red`/`white`)도 모델에 포함시키려면, 이 문자열 데이터를 숫자(보통 0과 1)로 변환해야 합니다. 이를 **더미 변수화(dummy variable)**라고 하며, `pandas`의 `get_dummies` 함수를 사용하면 쉽게 처리할 수 있습니다.

```python
wine_with_dummies = pd.get_dummies(wine, columns=['type'], drop_first=True)
# 'type' 열이 사라지고 'type_white' (white이면 1, red이면 0) 열이 생성됨
```

#### **5. 심화 내용 (모델 성능 평가와 변수 선택)**

`R-squared` 외에도 모델을 평가하는 지표는 많습니다(MSE, MAE, RMSE 등). 또한, `summary()` 결과에서 p-value가 높은(즉, 유의미하지 않은) 변수들은 모델에서 제거하여 더 간결하고 성능 좋은 모델을 만들 수 있습니다. 이러한 과정을 **변수 선택(Feature Selection)**이라고 하며, 후진 제거법, 전진 선택법 등 다양한 기법이 존재합니다. 이는 머신러닝 분야에서 매우 중요한 주제입니다.

---

### **Part 4: 데이터 시각화**

분석 결과와 데이터의 분포를 그래프로 표현하여 복잡한 정보를 직관적으로 이해하고 인사이트를 전달하는 과정입니다.

#### **1. 코드, 문법 및 개별 설명**

```python
# %%
import matplotlib.pyplot as plt
import seaborn as sns
import statsmodels.api as sm

# 한글 폰트 설정 (Windows '맑은 고딕' 기준)
plt.rc('font', family='Malgun Gothic')
plt.rcParams['axes.unicode_minus'] = False # 음수 부호 깨짐 방지

sns.set_style("darkgrid") # 그래프 배경 스타일 설정

# 레드 와인과 화이트 와인의 품질 분포를 히스토그램과 KDE(커널 밀도 추정) 곡선으로 시각화
plt.figure(figsize=(10, 6)) # 그래프 크기 설정
# 연산 과정 (sns.distplot):
# 1. red_wine_quality 데이터의 구간별 빈도를 계산하여 히스토그램을 그림
# 2. kde=True 옵션으로 히스토그램을 부드럽게 연결한 분포 곡선을 함께 그림
sns.distplot(red_wine_quality, kde=True, color="red", label="Red Wine")
sns.distplot(white_wine_quality, kde=True, color="skyblue", label="White Wine")

plt.title("와인 타입에 따른 품질 분포")
plt.legend() # 범례 표시
plt.show() # 그래프 출력

# 부분 회귀 플롯(Partial Regression Plot) 시각화
# 다른 모든 변수들의 영향을 통제(제거)했을 때, 특정 독립 변수 하나가 종속 변수에 미치는 순수한 관계를 보여줌
fig = plt.figure(figsize=(8, 13))

# regression_result 모델에 포함된 모든 독립 변수에 대해 부분 회귀 플롯을 한번에 그려줌
sm.graphics.plot_partregress_grid(regression_result, fig=fig)
plt.show()
# 결과 해석:
# 각 작은 그래프는 하나의 독립 변수와 quality의 관계를 보여줌.
# 예를 들어, alcohol 그래프에서 점들이 뚜렷한 우상향 추세를 보인다면,
# 다른 조건이 동일할 때 알코올 도수가 높을수록 품질이 높아지는 강한 양의 관계가 있음을 의미.
# 반면, 점들이 거의 수평으로 퍼져 있다면 해당 변수는 품질과 큰 관계가 없음을 시사.
```

#### **2. 해당 설명**

데이터 시각화는 분석의 결과를 효과적으로 전달하는 최종 단계입니다. `seaborn`의 `distplot` (최신 버전에서는 `histplot`)은 데이터의 분포를 한눈에 파악하는 데 매우 유용합니다. 위 히스토그램을 통해 우리는 화이트 와인이 레드 와인보다 특정 품질 등급(예: 6)에 더 많이 집중되어 있는 경향 등을 시각적으로 확인할 수 있습니다.

**부분 회귀 플롯**은 다중 회귀 분석 결과를 해석하는 데 매우 강력한 도구입니다. `summary()` 테이블의 숫자만으로는 파악하기 어려운 각 변수의 실제 영향력과 데이터의 패턴(선형성, 이상치 등)을 시각적으로 검토할 수 있게 해줍니다. 이 플롯들을 통해 우리는 **"와인의 품질에 가장 큰 영향을 미치는 요소는 알코올 도수와 휘발산(volatile_acidity)이다"** 와 같은 핵심적인 결론을 직관적으로 도출할 수 있습니다.

#### **3. 응용 가능한 예제**

**"고객 만족도에 영향을 미치는 요인 시각화"**

고객 만족도(종속 변수)와 서비스 품질, 배송 속도, 가격 합리성 등(독립 변수) 간의 회귀 분석을 수행한 후, `plot_partregress_grid`를 이용해 어떤 요인이 고객 만족도에 가장 큰 영향을 미치는지 시각적으로 표현하여 비즈니스 개선의 우선순위를 정할 수 있습니다.

#### **4. 추가하고 싶은 내용 (상관관계 히트맵)**

회귀 분석을 하기 전에 독립 변수들 간의 상관관계를 미리 파악하는 것이 중요합니다. 상관관계가 너무 높은 변수들이 함께 모델에 들어가면 **다중공선성(Multicollinearity)** 문제가 발생하여 모델의 안정성을 해칠 수 있습니다. `seaborn`의 `heatmap`을 사용하면 변수 간 상관계수를 색상으로 한눈에 파악할 수 있습니다.

```python
plt.figure(figsize=(12, 12))
sns.heatmap(data=wine.corr(), annot=True, cmap='coolwarm')
plt.show()
```

#### **5. 심화 내용 (잔차 분석, Residual Analysis)**

회귀 모델이 데이터를 얼마나 잘 설명하는지 평가하기 위해 **잔차(Residuals, 실제값 - 예측값)**를 분석하는 과정이 필수적입니다. 잔차는 특정 패턴 없이 무작위로 분포해야 좋은 모델이라고 할 수 있습니다. `statsmodels`는 잔차의 정규성(Q-Q plot), 등분산성 등을 검토할 수 있는 다양한 시각화 도구를 제공하며, 이는 모델의 신뢰도를 진단하는 데 매우 중요합니다.
